{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "runModelScript",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cindyhfls/NMA_DL_2021_project/blob/main/IndividualScriptTests/runModelScript.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idt_5qF9ogSJ"
      },
      "source": [
        "_____________\n",
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHRR0PV20BqZ",
        "cellView": "form"
      },
      "source": [
        "#@title Import matplotlib, class functions, and set up variables.\n",
        "from matplotlib import rcParams \n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "\n",
        "import torch\n",
        "import copy\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "import numpy as np\n",
        "import math\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from matplotlib import pyplot as plt\n",
        "from torch.utils.data.dataloader import default_collate\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "rcParams['figure.figsize'] = [20, 4]\n",
        "rcParams['font.size'] =15\n",
        "rcParams['axes.spines.top'] = False\n",
        "rcParams['axes.spines.right'] = False\n",
        "rcParams['figure.autolayout'] = True\n",
        "# Data Loading\n",
        "\n",
        "#@title Data retrieval\n",
        "import os, requests\n",
        "\n",
        "fname = []\n",
        "for j in range(3):\n",
        "  fname.append('steinmetz_part%d.npz'%j)\n",
        "url = [\"https://osf.io/agvxh/download\"]\n",
        "url.append(\"https://osf.io/uv3mw/download\")\n",
        "url.append(\"https://osf.io/ehmw2/download\")\n",
        "\n",
        "for j in range(len(url)):\n",
        "  if not os.path.isfile(fname[j]):\n",
        "    try:\n",
        "      r = requests.get(url[j])\n",
        "    except requests.ConnectionError:\n",
        "      print(\"!!! Failed to download data !!!\")\n",
        "    else:\n",
        "      if r.status_code != requests.codes.ok:\n",
        "        print(\"!!! Failed to download data !!!\")\n",
        "      else:\n",
        "        with open(fname[j], \"wb\") as fid:\n",
        "          fid.write(r.content)\n",
        "\n",
        "alldat = np.array([])\n",
        "for j in range(len(fname)):\n",
        "  alldat = np.hstack((alldat, np.load('steinmetz_part%d.npz'%j, allow_pickle=True)['dat']))\n",
        "\n",
        "#@title Print Keys\n",
        "print(alldat[0].keys())\n",
        "# @title Set random seed\n",
        "\n",
        "# @markdown Executing `set_seed(seed=seed)` you are setting the seed\n",
        "\n",
        "# for DL its critical to set the random seed so that students can have a\n",
        "# baseline to compare their results to expected results.\n",
        "# Read more here: https://pytorch.org/docs/stable/notes/randomness.html\n",
        "\n",
        "# Call `set_seed` function in the exercises to ensure reproducibility.\n",
        "import random\n",
        "import torch\n",
        "\n",
        "def set_seed(seed=None, seed_torch=True):\n",
        "  if seed is None:\n",
        "    seed = np.random.choice(2 ** 32)\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  if seed_torch:\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "  print(f'Random seed {seed} has been set.')\n",
        "\n",
        "\n",
        "# In case that `DataLoader` is used\n",
        "def seed_worker(worker_id):\n",
        "  worker_seed = torch.initial_seed() % 2**32\n",
        "  np.random.seed(worker_seed)\n",
        "  random.seed(worker_seed)\n",
        "#@title Define Steinmetz Class\n",
        "class SteinmetzSession:\n",
        "  data = []\n",
        "  binSize = 10\n",
        "  nTrials = []\n",
        "  nNeurons = []\n",
        "  trialLen = 0\n",
        "  trimStart = \"trialStart\"\n",
        "  trimEnd =  \"trialEnd\"\n",
        "  def __init__(self, dataIn):\n",
        "    self.data = copy.deepcopy(dataIn)\n",
        "    dims1 = np.shape(dataIn['spks'])\n",
        "    self.nTrials = dims1[1]\n",
        "    self.nNeurons = dims1[0]\n",
        "    self.trialLen = dims1[2]\n",
        "\n",
        "  def binData(self, binSizeIn): # Inputs: data, scalar for binning. Combines binSizeIn bins together to bin data smaller Ex. binSizeIn of 5 on the original dataset combines every 5 10 ms bins into one 50 ms bin across all trials.\n",
        "    varsToRebinSum = ['spks']\n",
        "    varsToRebinMean = ['wheel', 'pupil']\n",
        "    spikes = self.data['spks']\n",
        "    histVec = range(0,self.trialLen+1, binSizeIn)\n",
        "    spikesBin = np.zeros((self.nNeurons, self.nTrials, len(histVec)))\n",
        "    print(histVec)\n",
        "    for trial in range(self.nTrials):\n",
        "      spikes1 = np.squeeze(spikes[:,trial,:])\n",
        "      for time1 in range(len(histVec)-1):\n",
        "        spikesBin[:,trial, time1] = np.sum(spikes1[:, histVec[time1]:histVec[time1+1]-1], axis=1)\n",
        "\n",
        "    spikesBin = spikesBin[:,:,:-1]\n",
        "    self.data['spks'] = spikesBin\n",
        "    self.trialLen = len(histVec) -1\n",
        "    self.binSize = self.binSize*binSizeIn\n",
        "\n",
        "    \n",
        "    s = \"Binned spikes, turning a \" + repr(np.shape(spikes)) + \" matrix into a \" + repr(np.shape(spikesBin)) + \" matrix\"\n",
        "    print(s)\n",
        "\n",
        "  def plotTrial(self, trialNum): # Basic function to plot the firing rate during a single trial. Used for debugging trimming and binning\n",
        "    plt.imshow(np.squeeze(self.data['spks'][:,trialNum,:]), cmap='gray_r', aspect = 'auto')\n",
        "    plt.colorbar()\n",
        "    plt.xlabel(\"Time (bins)\")\n",
        "    plt.ylabel(\"Neuron #\")\n",
        "    \n",
        "  def realign_data_to_movement(self,length_time_in_ms): # input has to be n * nTrials * nbins\n",
        "    align_time_in_bins = np.round(self.data['response_time']/self.binSize*1000)+ int(500/self.binSize) # has to add 0.5 s because the first 0.5 s is pre-stimulus\n",
        "    length_time_in_bins = int(length_time_in_ms/self.binSize)\n",
        "    validtrials = self.data['response']!=0\n",
        "    maxtime = self.trialLen\n",
        "    newshape = (self.nNeurons,self.nTrials)\n",
        "    newshape+=(length_time_in_bins,)\n",
        "    newdata = np.empty(newshape)\n",
        "    for count,align_time_curr_trial in enumerate(align_time_in_bins):\n",
        "      if (validtrials[count]==0)|(align_time_curr_trial+length_time_in_bins>maxtime) :\n",
        "        validtrials[count] = 0\n",
        "      else:\n",
        "        newdata[:,count,:]= self.data['spks'][:,count,int(align_time_curr_trial):int(align_time_curr_trial)+length_time_in_bins]\n",
        "    # newdata = newdata[:,validtrials,:]\n",
        "    self.data['spks'] = newdata\n",
        "    # self.validtrials = validtrials\n",
        "    print('spikes aligned to movement, returning validtrials')\n",
        "    return validtrials\n",
        "  \n",
        "  def get_areas(self):\n",
        "    print(set(list(self.data['brain_area'])))\n",
        "\n",
        "  def extractROI(self, region): #### extract neurons from single region\n",
        "    rmrt=list(np.where(self.data['brain_area']!=region))[0]\n",
        "    print(f' removing data from {len(rmrt)} neurons not contained in {region} ')\n",
        "    self.data['spks']=np.delete(self.data['spks'],rmrt,axis=0)\n",
        "    neur=len(self.data['spks'])\n",
        "    print(f'neurons remaining in trial {neur}')\n",
        "    self.data['brain_area']=np.delete(self.data['brain_area'],rmrt,axis=0)\n",
        "    self.data['ccf']=np.delete(self.data['ccf'],rmrt,axis=0)\n",
        "    \n",
        "  def FlattenTs(self):\n",
        "    self.data['spks']=np.hstack(self.data['spks'][:])\n",
        "\n",
        "  def removeTrialAvgFR(self):\n",
        "    mFR = self.data['spks'].mean(1)\n",
        "    mFR = np.expand_dims(mFR, 1)\n",
        "    print(np.shape(self.data['spks']))\n",
        "    print(np.shape(mFR))\n",
        "    self.data['spks'] = self.data['spks'].astype(float)\n",
        "    self.data['spks'] -= mFR\n",
        "\n",
        "  def sqrt_norm(self):\n",
        "    self.data['spks'] = np.sqrt(self.data['spks'])  \n",
        "  \n",
        "  def permdims(self):\n",
        "    return torch.permute(torch.tensor(self.data['spks']),(2,1,0))\n",
        "\n",
        "  def smoothFR(self, smoothingWidth):# TODO: Smooth the data and save it back to the data structure\n",
        "    return 0\n",
        "\n",
        "#@title get input for network from session 31\n",
        "s31=SteinmetzSession(alldat[30])\n",
        "s31.sqrt_norm()\n",
        "s31.removeTrialAvgFR()\n",
        "validtrials = s31.realign_data_to_movement(500) # get 500 ms from movement time, \n",
        "# cannot get realign and binning to work the same time =[\n",
        "\n",
        "---------\n",
        "# Model\n",
        "class Net(nn.Module): # our model\n",
        "  def __init__(self, ncomp, NN1, NN2, bidi=True, dropout = 0):\n",
        "    super(Net, self).__init__()\n",
        "\n",
        "    # play with some of the options in the RNN!\n",
        "    self.rnn1 = nn.RNN(NN1, ncomp, num_layers = 1, dropout = 0, # MO\n",
        "                      bidirectional = bidi, nonlinearity = 'tanh')\n",
        "    self.rnn2 = nn.RNN(NN2,ncomp,num_layers = 1, dropout = 0, bidirectional = bidi, nonlinearity = 'tanh') #TH\n",
        "\n",
        "    if bidi == True:\n",
        "      self.fclatent = nn.Linear(ncomp*2,ncomp*2)\n",
        "    else: \n",
        "      self.fclatent = nn.Linear(ncomp,ncomp)\n",
        "\n",
        "    self.fc = nn.Linear(ncomp,NN1)\n",
        "\n",
        "  def forward(self, x0,x1):\n",
        "    y2 = self.rnn2(x0)[0] # ncomp TH\n",
        "    y =  self.rnn1(x1)[0] # ncomp MOs\n",
        "    y = y + self.fclatent(y2) # ncomp MOs with projection of latent TH components\n",
        "\n",
        "    if self.rnn1.bidirectional:\n",
        "      # if the rnn is bidirectional, it concatenates the activations from the forward and backward pass\n",
        "      # we want to add them instead, so as to enforce the latents to match between the forward and backward pass\n",
        "      q = (y[:, :, :ncomp] + y[:, :, ncomp:])/2\n",
        "    else:\n",
        "      q = y\n",
        "\n",
        "    # the softplus function is just like a relu but it's smoothed out so we can't predict 0\n",
        "    # if we predict 0 and there was a spike, that's an instant Inf in the Poisson log-likelihood which leads to failure\n",
        "    z = F.softplus(self.fc(q), 10)\n",
        "\n",
        "    return z, q\n",
        "\n",
        "def pearson_corr_tensor(input, output):\n",
        "  rpred = output.detach().cpu().numpy()\n",
        "  rreal = input.detach().cpu().numpy()\n",
        "  rpred_flat = np.ndarray.flatten(rpred)\n",
        "  rreal_flat = np.ndarray.flatten(rreal)\n",
        "  corrcoeff = np.corrcoef(rpred_flat, rreal_flat)\n",
        "  return corrcoeff[0,1]\n",
        "\n",
        "\n",
        "def runAreaModels(s31, lr, firstArea, secondArea, latentSize, nIter, validtrials, dropout, bidi, plotExamples = False):\n",
        "  ### print areas\n",
        "  s31.get_areas()\n",
        "\n",
        "  # s31.FlattenTs()\n",
        "  nTr = np.argwhere(validtrials) # since the other trials were defaulted to a zero value, only plot the valid trials\n",
        "\n",
        "  MO = copy.deepcopy(s31)\n",
        "  ###remove all neurons not in motor cortex\n",
        "  MO.extractROI(firstArea)\n",
        "  ### plot a trial from motor neuron\n",
        "  plt.figure()\n",
        "  MO.plotTrial(nTr[1])\n",
        "  plt.title(firstArea)\n",
        "  ### permute the trials\n",
        "  MOdata = MO.permdims().float().to(device)\n",
        "  MOdata = MOdata[:,validtrials,:]\n",
        "  print(MOdata.shape)\n",
        "\n",
        "  TH = copy.deepcopy(s31)\n",
        "  ###remove all neurons not in motor cortex\n",
        "  TH.extractROI(secondArea)\n",
        "  ### plot a trial from motor neuron\n",
        "  plt.figure()\n",
        "  TH.plotTrial(nTr[1])\n",
        "  plt.title(secondArea)\n",
        "\n",
        "  THdata = TH.permdims().float().to(device)\n",
        "  THdata = THdata[:,validtrials,:]\n",
        "\n",
        "  NN1 = MOdata.shape[2]\n",
        "  NN2 = THdata.shape[2]\n",
        "\n",
        "  N = MOdata.shape[1]\n",
        "  np.random.seed(42)\n",
        "  ii = torch.randperm(N).tolist()\n",
        "  idx_train = ii[:math.floor(0.6*N)]\n",
        "  idx_val = ii[math.floor(0.6*N):math.floor(0.9*N)]\n",
        "  idx_test = ii[math.floor(0.9*N):]\n",
        "\n",
        "  x0_train = THdata[:,idx_train,:]\n",
        "  x0_val = THdata[:,idx_val,:]\n",
        "  x0_test = THdata[:,idx_test,:]\n",
        "\n",
        "  x1_train = MOdata[:,idx_train,:]\n",
        "  x1_val = MOdata[:,idx_val,:]\n",
        "  x1_test = MOdata[:,idx_test,:]\n",
        "  ncomp = latentSize\n",
        "\n",
        "  learning_rate_start = lr\n",
        "  net_baseline = Net(ncomp, NN1, NN2, bidi = bidi, dropout= dropout).to(device)\n",
        "  net_baseline.fclatent.weight.data[:] = 0 # fixed weights =0 so the TH input is not considered\n",
        "  net_baseline.fclatent.bias.data[:] = 0\n",
        "  net_baseline.fclatent.weight.requires_grad = False\n",
        "  net_baseline.fclatent.bias.requires_grad = False\n",
        "\n",
        "  # special thing:  we initialize the biases of the last layer in the neural network\n",
        "  # we set them as the mean firing rates of the neurons.\n",
        "  # this should make the initial predictions close to the mean, because the latents don't contribute much\n",
        "  # net_baseline.fc.bias.data[:] = MOdata.mean((0,1))\n",
        "\n",
        "  # we set up the optimizer later in the training loop\n",
        "\n",
        "  print(net_baseline)\n",
        "  loss = nn.MSELoss()\n",
        "  optimizer = torch.optim.Adam(net_baseline.parameters(), lr=learning_rate_start)\n",
        "\n",
        "  training_cost = []\n",
        "  val_cost = []\n",
        "  for k in range(nIter):\n",
        "    ### training\n",
        "    optimizer.zero_grad()\n",
        "    # the network outputs the single-neuron prediction and the latents\n",
        "    z, y = net_baseline(x0_train,x1_train)\n",
        "    cost = loss(z,x1_train).mean()\n",
        "    # # our log-likelihood cost\n",
        "    # cost = Poisson_loss(z, x1_train).mean()\n",
        "    # train the network as usual\n",
        "    cost.backward()\n",
        "    optimizer.step()\n",
        "    training_cost.append(cost.item())\n",
        "\n",
        "    ### test on validation data\n",
        "    z_val,_ = net_baseline(x0_val,x1_val)\n",
        "    cost = loss(z_val,x1_val).mean()\n",
        "    # cost = Poisson_loss(z_val, x1_val).mean()\n",
        "    val_cost.append(cost.item())\n",
        "\n",
        "    if k % 100 == 0:\n",
        "      print(f'iteration {k}, cost {cost.item():.4f}')\n",
        "\n",
        "  if plotExamples:\n",
        "    plt.plot(training_cost,'b')\n",
        "    plt.plot(val_cost,'r')\n",
        "    plt.hlines(np.min(training_cost),0,nIter,'b',linestyles = '--')\n",
        "    plt.hlines(np.min(val_cost),0,nIter,'r',linestyles = '--')\n",
        "\n",
        "    plt.legend(['training cost','validation cost','min training cost','min validation cost'])\n",
        "    plt.title('Training cost over epochs')\n",
        "    plt.ylabel('cost')\n",
        "    plt.xlabel('epochs')\n",
        "\n",
        "  corr = pearson_corr_tensor(x1_val, z_val)\n",
        "\n",
        "  rpred = z.detach().cpu().numpy()\n",
        "  rates = x1_train.cpu()\n",
        "\n",
        "  if plotExamples:\n",
        "    nTr = 5\n",
        "    nNeuron = 0\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(rates[:,nTr, nNeuron])\n",
        "    plt.plot(rpred[:,nTr, nNeuron])\n",
        "\n",
        "    plt.legend(['spikes', 'rates (predicted)'])\n",
        "    plt.title(f'training set Trial {nTr}, Neuron {nNeuron}')\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure(figsize = (12, 8))\n",
        "    plt.subplot(121)\n",
        "    plt.imshow(rates[:, nTr, :].T, cmap='gray_r')\n",
        "    plt.xlabel('Time (ms)')\n",
        "    plt.ylabel('Cell #')\n",
        "    plt.title(f'True rates (training set trial {nTr})')\n",
        "\n",
        "    plt.subplot(122)\n",
        "    plt.imshow(rpred[:, nTr, :].T, cmap='gray_r')\n",
        "    plt.xlabel('Time (ms)')\n",
        "    plt.ylabel('Cell #')\n",
        "    plt.title(f'Inferred rates (training set trial {nTr})')\n",
        "    plt.show()\n",
        "\n",
        "  PATH = 'steinmetz_model_baseline.pt'\n",
        "  torch.save(net_baseline.state_dict(), PATH) \n",
        "  del net_baseline\n",
        "\n",
        "  # load saved model\n",
        "  net_baseline = Net(ncomp, NN1, NN2, bidi = bidi, dropout= dropout).to(device)\n",
        "  net_baseline.load_state_dict(torch.load('steinmetz_model_baseline.pt'))\n",
        "\n",
        "  # after training the baseline network, get the weights of rnn1 and freeze it\n",
        "  net_withinput  = copy.deepcopy(net_baseline)\n",
        "\n",
        "  net_withinput.fclatent.weight.requires_grad = True\n",
        "  net_withinput.fclatent.bias.requires_grad = True\n",
        "\n",
        "  # # set weight initalization to random\n",
        "  net_withinput.fclatent.reset_parameters()\n",
        "\n",
        "\n",
        "  net_withinput.rnn1.weight_ih_l0.requires_grad = False\n",
        "  net_withinput.rnn1.weight_hh_l0.requires_grad = False\n",
        "  net_withinput.rnn1.bias_ih_l0.requires_grad = False\n",
        "  net_withinput.rnn1.bias_hh_l0.requires_grad = False\n",
        "  if bidi:\n",
        "    net_withinput.rnn1.weight_ih_l0_reverse.requires_grad = False\n",
        "    net_withinput.rnn1.weight_hh_l0_reverse.requires_grad = False\n",
        "    net_withinput.rnn1.bias_ih_l0_reverse.requires_grad = False\n",
        "    net_withinput.rnn1.bias_hh_l0_reverse.requires_grad = False\n",
        "        \n",
        "  print(net_withinput)\n",
        "\n",
        "\n",
        "  # we define the Poisson log-likelihood loss\n",
        "  # def Poisson_loss(lam, spk):\n",
        "  #   return lam - spk * torch.log(lam)\n",
        "  loss = nn.MSELoss()\n",
        "  optimizer = torch.optim.Adam(net_withinput.parameters(), lr=learning_rate_start) # this is very important\n",
        "\n",
        "\n",
        "\n",
        "  training_cost2 = []\n",
        "  val_cost2 = []\n",
        "  for k in range(nIter):\n",
        "    ### training\n",
        "    optimizer.zero_grad()\n",
        "    # the network outputs the single-neuron prediction and the latents\n",
        "    z, y = net_withinput(x0_train,x1_train)\n",
        "    # our log-likelihood cost\n",
        "    cost = loss(z, x1_train).mean()\n",
        "    # train the network as usual\n",
        "    cost.backward()\n",
        "    optimizer.step()\n",
        "    training_cost2.append(cost.item())\n",
        "\n",
        "    ### test on validation data\n",
        "    z_val,_ = net_withinput(x0_val,x1_val)\n",
        "    cost = loss(z_val, x1_val).mean()\n",
        "    val_cost2.append(cost.item())\n",
        "\n",
        "    if k % 100 == 0:\n",
        "      print(f'iteration {k}, cost {cost.item():.4f}')\n",
        "\n",
        "  corr2 = pearson_corr_tensor(x1_val, z_val)\n",
        "\n",
        "  return training_cost, training_cost2, val_cost, val_cost2, corr, corr2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKPYawfX7qpJ"
      },
      "source": [
        "\n",
        "lr = 0.002\n",
        "firstArea = 'MOs'\n",
        "secondArea = 'TH'\n",
        "latentSize = 3\n",
        "nIter = 1000\n",
        "dropout = .5\n",
        "bidi = False\n",
        "train_cost, train_cost2, val_cost, val_cost2, corr, corr2 = runAreaModels(s31, lr, firstArea, secondArea, latentSize, nIter, validtrials, dropout, bidi)\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(train_cost)\n",
        "plt.plot(train_cost2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XRIIHV8rRxo"
      },
      "source": [
        "--------------------- The end -------------------\n"
      ]
    }
  ]
}